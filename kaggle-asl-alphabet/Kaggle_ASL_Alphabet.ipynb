{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle ASL Alphabet",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDwJO9lsbuTv",
        "colab_type": "text"
      },
      "source": [
        "The data for this notebook comes from https://www.kaggle.com/grassknoted/asl-alphabet/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vhw70_FbDcp",
        "colab_type": "code",
        "outputId": "da45452c-da1e-4091-808d-509846fc7886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "  1. Mount Google Drive (no need to check if already mounted, it does that for you)\n",
        "  2. \n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_dir = \"/content/gdrive/My Drive/CS474 Final Project/Kaggle ASL Alphabet/\"\n",
        "train_dir = \"asl_alphabet_train/asl_alphabet_train\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak1Cqg7cP-kF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "4a74d618-3c80-43f6-f71d-d69c6f752798"
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.1+cu100)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.3)\n",
            "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.0+cu100)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3b28f5c4c813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0m__ITB__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoFormattedTB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Verbose'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor_scheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LightBg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You need to request a GPU from Runtime > Change Runtime\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: You need to request a GPU from Runtime > Change Runtime"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8oL6AqMPoAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ASLDataset(Dataset):\n",
        "  def to_one_hot(self, class_index):\n",
        "    oh = torch.zeros((len(self.dataset_folder)))\n",
        "    oh[class_index] = 1\n",
        "    return oh\n",
        "    \n",
        "  def __init__(self, root_path, train_path, size=512):\n",
        "      self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root_path, train_path) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    sample = self.dataset_folder[index]\n",
        "    return sample[0], self.to_one_hot(sample[1])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb5vda7aQSwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = ASLDataset(root_dir, train_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZpQblwUQgTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "9b3c7c10-35f0-4b23-b035-8d672a2641f6"
      },
      "source": [
        "batch_size = 10\n",
        "validation_split = 0.2\n",
        "shuffle_dataset = True\n",
        "random_seed= 12\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-db6492bd4c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Creating PT data samplers and loaders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvalid_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SubsetRandomSampler' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKvaGpQVRFfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "34abb611-43a1-4352-c200-9088f2119413"
      },
      "source": [
        "# This is what was talked about in the video for memory management\n",
        "num_epochs = 6\n",
        "valid_frequency = 10\n",
        "\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    #your code for calling dataset and dataloader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    loop = tqdm(\n",
        "        total=(len(train_loader) * num_epochs) +\n",
        "          (len(validation_loader) * (num_epochs // valid_frequency))\n",
        "        , position = 0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      sum_loss = 0\n",
        "      count_loss = 0\n",
        "\n",
        "      for x, y_truth in train_loader:\n",
        "        x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x)\n",
        "        y_long = y_truth.long()\n",
        "\n",
        "        loss = objective(y_hat, y_long)\n",
        "        sum_loss += loss.item()\n",
        "        count_loss += 1\n",
        "\n",
        "        loop.set_description(\"epoch:{}/{}, loss:{:.4f}, accuracy:{:.4f}.\".format(epoch,batch, loss.item(), accuracy))\n",
        "        loop.update(1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      train_losses.append((epoch, sum_loss / count_loss))\n",
        "\n",
        "      if epoch % valid_frequency == 0:\n",
        "        sum_loss = 0\n",
        "        count_loss = 0\n",
        "        with torch.no_grad():\n",
        "          for x, y_truth in valid_loader:\n",
        "            x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "            y_hat = model(x)\n",
        "            loss = objective(y_hat, y_truth.long())\n",
        "            sum_loss += loss.item()\n",
        "            count_loss += 1       \n",
        "          validation_losses.append((epoch, sum_loss / count_loss))\n",
        "            # validation_accuracies.append([total_batch_counter, sum_acc / count])\n",
        "          \n",
        "    # Call your model, figure out loss and accuracy\n",
        "    \n",
        "  except:\n",
        "    __ITB__()\n",
        "    \n",
        "scope()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNj0-7ePcRF",
        "colab_type": "text"
      },
      "source": [
        "# SSH Access\n",
        "\n",
        "If ssh_connect is True, an ssh server will be set up on the hosting server. Ergo power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmJosbn3c0i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssh_connect = False\n",
        "if ssh_connect:\n",
        "  import random, string, getpass\n",
        "\n",
        "  password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n",
        "  alias = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(8))\n",
        "  ! echo root:$password | chpasswd\n",
        "\n",
        "  ! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "  ! mkdir -p /var/run/sshd\n",
        "  ! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config && echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "  ! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc && echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n",
        "  get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
        "\n",
        "  print('sshpass -p {} ssh -o \"StrictHostKeyChecking no\" -J serveo.net root@{}'.format(password, alias))\n",
        "  ! ssh -o \"StrictHostKeyChecking no\" -R $alias:22:localhost:22 serveo.net"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}