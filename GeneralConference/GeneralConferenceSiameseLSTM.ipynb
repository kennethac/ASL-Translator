{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneralConference.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te3Bjkwhe2Ue",
        "colab_type": "text"
      },
      "source": [
        "This notebook aims to recreate the MaLSTM model for comparing sentences found  [here](https://github.com/likejazz/Siamese-LSTM). It utilizes the Quora question pairs dataset for training and saves out the model so that it can be used in comparing General Conference translations as a loss function.\n",
        "\n",
        "Remember that there are some IP claims on the data and we can't actually use it for anything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPsFv-ccwAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl -o gc.zip https://students.cs.byu.edu/~kac1995/2000.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOllGGoO0edW",
        "colab_type": "code",
        "outputId": "1ead1c52-da4b-4df5-aa28-0b766b255b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "  1. Mount Google Drive (no need to check if already mounted, it does that for you)\n",
        "  2. \n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPzYICbu41Vr",
        "colab_type": "code",
        "outputId": "15fdc38a-1e89-472d-9c33-73f1682644f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'kennethchristensen'\n",
        "os.environ['KAGGLE_KEY'] = '20890c9bf813dceb31ed6b65d2210769'\n",
        "\n",
        "!kaggle competitions download -c quora-question-pairs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw1VuP73HHzz",
        "colab_type": "code",
        "outputId": "3a105574-c9cc-49c7-e011-a941a5c845e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dY2lQlOaGCPB",
        "colab": {}
      },
      "source": [
        "# root directory has a structure like \"year/month/talk/[video|text]\"\n",
        "train_file = \"/content/train.csv\"\n",
        "test_file = \"/content/test.csv\"\n",
        "output_dir = \"/content/gdrive/My Drive/CS474 Final Project/MaLSTM\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgRirH6MHIbf",
        "colab_type": "code",
        "outputId": "0c2f4ef8-802b-4fbd-9bcf-61cbc551a156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install tqdm\n",
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!pip3 install gensim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.3.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.10.36)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.13.36)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_UyQQ2HI4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchtext\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import gc\n",
        "import itertools\n",
        "\n",
        "import os\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import gensim.downloader as gensim_api\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_mlV8yL-Kc",
        "colab_type": "code",
        "outputId": "e0f70a6f-3234-4253-bc62-27ac793b76b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvOPp1Jol5K",
        "colab_type": "code",
        "outputId": "07ccb81a-b754-4576-f7fb-7ba7ae2ad9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "model_glove_wikipedia = gensim_api.load(\"glove-wiki-gigaword-100\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO76qQ-xHmam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(train_file)\n",
        "for q in ['question1', 'question2']:\n",
        "    train_df[q + '_n'] = train_df[q]\n",
        "\n",
        "# Make GloVE embeddings\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = spacy.load('en').tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psVDJANiMrtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTyCqwnHer4C",
        "colab_type": "code",
        "outputId": "88105b25-7307-4df0-fb3a-e1388a7c07f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "      <th>question1_n</th>\n",
              "      <th>question2_n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                        question2_n\n",
              "0   0  ...  What is the step by step guide to invest in sh...\n",
              "1   1  ...  What would happen if the Indian government sto...\n",
              "2   2  ...  How can Internet speed be increased by hacking...\n",
              "3   3  ...  Find the remainder when [math]23^{24}[/math] i...\n",
              "4   4  ...            Which fish would survive in salt water?\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aVFhCUAIbAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_glove_embeddings(df, embedding_dim=100):\n",
        "  vocabs = {}\n",
        "  vocabs_cnt = 0\n",
        "\n",
        "  stops = set(stopwords.words('english'))\n",
        "  for index, row in df.iterrows():\n",
        "    if index != 0 and index % 10000 == 0:\n",
        "      print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
        "\n",
        "    for question in ['question1', 'question2']:\n",
        "      q2n = [] # q2n -> question numbers representation\n",
        "      for word in tokenizer(row[question]):\n",
        "        if word in stops:\n",
        "          continue\n",
        "\n",
        "        if word not in vocabs:\n",
        "          vocabs_cnt += 1\n",
        "          vocabs[word] = vocabs_cnt\n",
        "          q2n.append(vocabs_cnt)\n",
        "        else:\n",
        "          q2n.append(vocabs[word])\n",
        "\n",
        "      df.at[index, question + '_n'] = np.asarray(q2n)\n",
        "  \n",
        "  embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "  embeddings[0] = 0 # Whatever padding?\n",
        "\n",
        "  for word, index in vocabs.items():\n",
        "    if word in model_glove_wikipedia.vocab:\n",
        "      embeddings[index] = model_glove_wikipedia[word]\n",
        "  \n",
        "  return df, torch.tensor(embeddings)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU65y7I9LaaR",
        "colab_type": "code",
        "outputId": "2e19fb6e-b36f-487e-c7cd-621aa2d46706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "train_df, embeddings = make_glove_embeddings(train_df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10,000 sentences embedded.\n",
            "20,000 sentences embedded.\n",
            "30,000 sentences embedded.\n",
            "40,000 sentences embedded.\n",
            "50,000 sentences embedded.\n",
            "60,000 sentences embedded.\n",
            "70,000 sentences embedded.\n",
            "80,000 sentences embedded.\n",
            "90,000 sentences embedded.\n",
            "100,000 sentences embedded.\n",
            "110,000 sentences embedded.\n",
            "120,000 sentences embedded.\n",
            "130,000 sentences embedded.\n",
            "140,000 sentences embedded.\n",
            "150,000 sentences embedded.\n",
            "160,000 sentences embedded.\n",
            "170,000 sentences embedded.\n",
            "180,000 sentences embedded.\n",
            "190,000 sentences embedded.\n",
            "200,000 sentences embedded.\n",
            "210,000 sentences embedded.\n",
            "220,000 sentences embedded.\n",
            "230,000 sentences embedded.\n",
            "240,000 sentences embedded.\n",
            "250,000 sentences embedded.\n",
            "260,000 sentences embedded.\n",
            "270,000 sentences embedded.\n",
            "280,000 sentences embedded.\n",
            "290,000 sentences embedded.\n",
            "300,000 sentences embedded.\n",
            "310,000 sentences embedded.\n",
            "320,000 sentences embedded.\n",
            "330,000 sentences embedded.\n",
            "340,000 sentences embedded.\n",
            "350,000 sentences embedded.\n",
            "360,000 sentences embedded.\n",
            "370,000 sentences embedded.\n",
            "380,000 sentences embedded.\n",
            "390,000 sentences embedded.\n",
            "400,000 sentences embedded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqLVyHAjPf5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_and_zero_padding(df):\n",
        "    # Split to dicts\n",
        "    # print(\"Type of df['question1_n'] {}\".format(type(df['question1_n'])))\n",
        "    left = [ torch.zeros((300)) ] + [torch.tensor(i) for i in df['question1_n'].values]\n",
        "    right = [ torch.zeros((300)) ] + [torch.tensor(i) for i in df['question2_n'].values]\n",
        "\n",
        "    left = torch.nn.utils.rnn.pad_sequence(left)\n",
        "    right = torch.nn.utils.rnn.pad_sequence(right)\n",
        "    \n",
        "    return { \"left\": left[:,1:], \"right\": right[:,1:] }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAVDF0tlOC95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split to train validation\n",
        "validation_size = int(len(train_df) * 0.1)\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[['question1_n', 'question2_n']]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmDWZ1JLPgol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = split_and_zero_padding(X_train)\n",
        "X_validation = split_and_zero_padding(X_validation)\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prqplo57jNl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceDataset(Dataset):\n",
        "  def __init__(self, sentences, labels):\n",
        "    self.sentences = sentences\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.sentences['left'].shape[1]\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return self.sentences['left'][:,i], self.sentences['right'][:,i], self.labels[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdi4BeGXO02",
        "colab_type": "code",
        "outputId": "1602e4f5-6aa6-485c-ddf3-dfa98efdf4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(\"{} {}\".format(X_train['left'].shape, X_train['right'].shape))\n",
        "print(\"{} {}\".format(len(X_train['left']), len(Y_train)))\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert X_train['left'].shape[1] == len(Y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([300, 363859]) torch.Size([300, 363859])\n",
            "300 363859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLs5ILCSay6G",
        "colab_type": "code",
        "outputId": "66aebc46-f78b-4f00-d671-180a83fcf374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embeddings[[1,3,3]].shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bDZwr6UbCdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingLookupModule(nn.Module):\n",
        "  def forward(self, l):\n",
        "    l = l.int().numpy()\n",
        "    # print(type(l))\n",
        "    # print(\"l size: {}\".format(l.shape))\n",
        "    res = torch.tensor(embeddings[l]).float().transpose(0,1)\n",
        "    # print(\"Looked up size: {}\".format(res.size()))\n",
        "    return res.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYjHzhEHajIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    EmbeddingLookupModule(),\n",
        "    nn.LSTM(input_size=100, hidden_size=200)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAVZ-S7Hea66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_manhattan(h1, h2):\n",
        "  return torch.exp(-torch.sum(torch.abs(h1 - h2), axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFbV3v9BiyzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 50\n",
        "batch_size = 1024\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60n-kmB0lypU",
        "colab_type": "code",
        "outputId": "433c6135-e207-46c3-ccce-2404030a7a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Y_train\n",
        "X_train['left'].size()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300, 363859])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmSXbOXtjll9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SentenceDataset(X_train, Y_train)\n",
        "valid_dataset = SentenceDataset(X_validation, Y_validation)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVifWkNBkwRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X6w7k1dmfDH",
        "colab_type": "code",
        "outputId": "2adf0bfa-1df6-403f-8c2e-4a89f73e8598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset[0][0].size()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPM6gjfPpL48",
        "colab_type": "code",
        "outputId": "7c6c9b48-7ace-4556-8c99-dd752abd369c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "363859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgvbA09hYgnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06kaAFWrkAW7",
        "colab_type": "code",
        "outputId": "b1f18184-7ad8-4eb9-868d-ee293741f931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "lowest_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  tot = 0\n",
        "  cnt = 0\n",
        "  acc_tot = 0\n",
        "  acc_cnt = 0\n",
        "\n",
        "  for q1, q2, y in train_loader:\n",
        "    y = y.cuda(async=False)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    _, (h1, _) = model(q1)\n",
        "    _, (h2, _) = model(q2)\n",
        "    \n",
        "    dist = compute_manhattan(h1, h2).squeeze(0)\n",
        "\n",
        "    # print(h1.size())\n",
        "    # print(dist.size())\n",
        "    # print(y.size())\n",
        "    # print(y)\n",
        "\n",
        "    loss = objective(dist, y.float())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
        "\n",
        "    if loss.item() < lowest_loss:\n",
        "      torch.save(model.state_dict(), os.path.join(output_dir, \"e{}_l{}.mod\".format(epoch, loss.item())))\n",
        "      lowest_loss = loss.item()\n",
        "    tot += loss.item()\n",
        "    cnt += 1\n",
        "\n",
        "    accuracy = 1- (torch.nonzero(dist - y).size(0) / len(y))\n",
        "    acc_tot += accuracy\n",
        "    acc_cnt += 1\n",
        "  train_acc.append((epoch, acc_tot / acc_cnt))\n",
        "  train_losses.append((epoch, tot / cnt))\n",
        "\n",
        "  if epoch != 0 and epoch % 10 == 0:\n",
        "    tot = 0\n",
        "    cnt = 0\n",
        "    acc_tot = 0\n",
        "    acc_cnt = 0\n",
        "    with torch.no_grad():\n",
        "      for q1, q2, y in valid_loader:\n",
        "        y = y.cuda(async=False)\n",
        "        _, (h1, _) = model(q1)\n",
        "        _, (h2, _) = model(q2)\n",
        "        \n",
        "        dist = compute_manhattan(h1, h2).squeeze(0)\n",
        "        loss = objective(dist, y.float())\n",
        "\n",
        "        print(\"Epoch: {}, Valid Loss: {}\".format(epoch, loss.item()))\n",
        "        tot += loss.item()\n",
        "        cnt += 1\n",
        "\n",
        "        accuracy = 1 - (torch.nonzero(dist - y).size(0) / len(y))\n",
        "        acc_tot += accuracy\n",
        "        acc_cnt += 1\n",
        "      valid_acc.append((epoch, acc_tot / acc_cnt))\n",
        "      validation_losses.append((epoch, tot / cnt))\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.6123043298721313\n",
            "Epoch: 0, Loss: 0.6328122019767761\n",
            "Epoch: 0, Loss: 0.6503902673721313\n",
            "Epoch: 0, Loss: 0.6308589577674866\n",
            "Epoch: 0, Loss: 0.6269527077674866\n",
            "Epoch: 0, Loss: 0.6347652077674866\n",
            "Epoch: 0, Loss: 0.6367183923721313\n",
            "Epoch: 0, Loss: 0.6572262048721313\n",
            "Epoch: 0, Loss: 0.6484371423721313\n",
            "Epoch: 0, Loss: 0.6210933327674866\n",
            "Epoch: 0, Loss: 0.6328121423721313\n",
            "Epoch: 0, Loss: 0.6513667702674866\n",
            "Epoch: 0, Loss: 0.6396480202674866\n",
            "Epoch: 0, Loss: 0.6181636452674866\n",
            "Epoch: 0, Loss: 0.6259761452674866\n",
            "Epoch: 0, Loss: 0.6328121423721313\n",
            "Epoch: 0, Loss: 0.6269527673721313\n",
            "Epoch: 0, Loss: 0.6230464577674866\n",
            "Epoch: 0, Loss: 0.6328120827674866\n",
            "Epoch: 0, Loss: 0.6425777077674866\n",
            "Epoch: 0, Loss: 0.6445308327674866\n",
            "Epoch: 0, Loss: 0.6533198952674866\n",
            "Epoch: 0, Loss: 0.6562496423721313\n",
            "Epoch: 0, Loss: 0.6201167702674866\n",
            "Epoch: 0, Loss: 0.6435542702674866\n",
            "Epoch: 0, Loss: 0.6357417702674866\n",
            "Epoch: 0, Loss: 0.6152339577674866\n",
            "Epoch: 0, Loss: 0.6259761452674866\n",
            "Epoch: 0, Loss: 0.6123043298721313\n",
            "Epoch: 0, Loss: 0.6074215173721313\n",
            "Epoch: 0, Loss: 0.6298823952674866\n",
            "Epoch: 0, Loss: 0.6484370827674866\n",
            "Epoch: 0, Loss: 0.6738277077674866\n",
            "Epoch: 0, Loss: 0.6064448952674866\n",
            "Epoch: 0, Loss: 0.6298823952674866\n",
            "Epoch: 0, Loss: 0.6113277077674866\n",
            "Epoch: 0, Loss: 0.6132808923721313\n",
            "Epoch: 0, Loss: 0.6552730202674866\n",
            "Epoch: 0, Loss: 0.6367183327674866\n",
            "Epoch: 0, Loss: 0.6201167702674866\n",
            "Epoch: 0, Loss: 0.6308589577674866\n",
            "Epoch: 0, Loss: 0.6376948952674866\n",
            "Epoch: 0, Loss: 0.6249995827674866\n",
            "Epoch: 0, Loss: 0.6005855202674866\n",
            "Epoch: 0, Loss: 0.6347652077674866\n",
            "Epoch: 0, Loss: 0.6142573952674866\n",
            "Epoch: 0, Loss: 0.6513667702674866\n",
            "Epoch: 0, Loss: 0.6562495827674866\n",
            "Epoch: 0, Loss: 0.6533198952674866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urSzntZBbhoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_module = EmbeddingLookupModule()\n",
        "# test_lstm = nn.LSTM()\n",
        "inp = X_train['left']\n",
        "# print(test_module(inp).size())\n",
        "\n",
        "print(inp.size())\n",
        "model(inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3ELFRic6SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_emb(embeddings):\n",
        "  results = []\n",
        "  for e in embeddings.numpy():\n",
        "    top_choices = model_glove_wikipedia.similar_by_vector(e)\n",
        "    words, weights = zip(*top_choices)\n",
        "    weights = np.asarray(weights)\n",
        "    weights /= np.sum(weights)\n",
        "    word = np.random.choice(words, p=weights)\n",
        "    results.append(word)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTxKl2XiSBGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv0c-Pyvse9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.cuda()\n",
        "f.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7_2Dudskb4",
        "colab_type": "code",
        "outputId": "cfaae408-02a3-48b4-e7a7-e7a14a3c005f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "f[0:100].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UksgRU8UuslX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scope(s):\n",
        "  try:\n",
        "    #your code for calling dataset and dataloader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model(f[0:s].cuda())\n",
        "  except:\n",
        "    __ITB__()\n",
        "    raise\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUHsLDNFvUGs",
        "colab_type": "code",
        "outputId": "a608182d-f6ed-49d4-9d1b-e26b5ca8ba9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "scope(100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 3, 224, 224])\n",
            "torch.Size([100, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnq-m6W7vU_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZvfv2GNvm2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EapUalt4vpNN",
        "colab_type": "code",
        "outputId": "b346c31f-6216-4e22-f75f-8b219291f76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "import torch.cuda as cutorch\n",
        "\n",
        "for i in range(cutorch.device_count()):\n",
        "     if cutorch.getMemoryUsage(i) > MEM: \n",
        "         opts.gpuID = i\n",
        "         break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-95c24d6192bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m      \u001b[0;32mif\u001b[0m \u001b[0mcutorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMemoryUsage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mMEM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m          \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpuID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'getMemoryUsage'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X7bjEsjuGj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FkduX1Wv_eK",
        "colab_type": "code",
        "outputId": "3d4d8506-be01-428d-c8a8-2fc4a6c04982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install gputil\n",
        "import GPUtil as GPU\n",
        "\n",
        "def check_gpu():\n",
        "  GPUs = GPU.getGPUs()\n",
        "  gpu = GPUs[0]\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "check_gpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "GPU RAM Free: 6877MB | Used: 9403MB | Util  58% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pcwtP_a9pk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}