{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneralConference.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te3Bjkwhe2Ue",
        "colab_type": "text"
      },
      "source": [
        "This notebook uses data scraped from the churchofjesuschrist.org website with ASL language general conference talks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPsFv-ccwAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl -o gc.zip https://students.cs.byu.edu/~kac1995/2000.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOllGGoO0edW",
        "colab_type": "code",
        "outputId": "968ecdc5-9cc5-4f52-af8a-2800966288cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\"\"\"\n",
        "  1. Mount Google Drive (no need to check if already mounted, it does that for you)\n",
        "  2. \n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPYkGq9k0nZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf \"/content/gc\"\n",
        "!unzip \"/content/gdrive/My Drive/CS474 Final Project/GC/2000.zip\" > /dev/null\n",
        "!mv \"/content/users/guest/k/kac1995/dev/CS474-General-Conference-Downloader/gc\" \"/content/gc\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN_UowwLoUzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# root directory has a structure like \"year/month/talk/[video|text]\"\n",
        "root_dir = \"/content/gc\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgRirH6MHIbf",
        "colab_type": "code",
        "outputId": "6e757603-85da-442f-b245-e1c670c05040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "!sudo apt install libavdevice-dev libavfilter-dev > /dev/null # Required to get av to install\n",
        "!pip3 install av # Required for torchvision to work with videos.\n",
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!pip3 install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Collecting av\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/77/0be0fdaa3b7912c184705a4545ae6f1e9e47ab9e3834a3ef5caf2d7ca1e7/av-6.2.0.tar.gz (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: av\n",
            "  Building wheel for av (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for av: filename=av-6.2.0-cp36-cp36m-linux_x86_64.whl size=4976067 sha256=3a6d103edec90a8cdc9b4fd7d656504a8935868835aa1ed9b0dfbaad2c496d72\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c1/b2/05e83d944cde5df317e4542082d67756ec4224c7885aee2d66\n",
            "Successfully built av\n",
            "Installing collected packages: av\n",
            "Successfully installed av-6.2.0\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.3.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.10.32)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.13.32)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_UyQQ2HI4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, utils, datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchtext\n",
        "import spacy\n",
        "import gc\n",
        "import os\n",
        "import math\n",
        "import av\n",
        "import re\n",
        "import gensim.downloader as gensim_api\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FkduX1Wv_eK",
        "colab_type": "code",
        "outputId": "3d4d8506-be01-428d-c8a8-2fc4a6c04982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install gputil\n",
        "import GPUtil as GPU\n",
        "\n",
        "def clean():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "def check_gpu():\n",
        "  GPUs = GPU.getGPUs()\n",
        "  gpu = GPUs[0]\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "clean()\n",
        "check_gpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "GPU RAM Free: 6877MB | Used: 9403MB | Util  58% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvOPp1Jol5K",
        "colab_type": "code",
        "outputId": "d4b65925-4eb8-40c3-c483-d9b9339fcd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model_glove_wikipedia = gensim_api.load(\"glove-wiki-gigaword-100\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDfXDVZUplq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_length = len(model_glove_wikipedia[\"jesus\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0RMpJBKLAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_minutes = 1\n",
        "fps = 10\n",
        "sample_frames = sample_minutes * 60 * fps\n",
        "\n",
        "text_replacements = {\n",
        "    # end of paragraphs that may not have been done correctly\n",
        "    '\\.([^ ])': \". \\\\1\",\n",
        "    \"\\!([^ ])\": \"! \\\\1\",\n",
        "    \"\\?([^ ])\": \"? \\\\1\",\n",
        "    \"\\:([^ ])\": \": \\\\1\",\n",
        "    \n",
        "    # Some unicode chars that I know of\n",
        "    u\"\\u201c\": '\"',\n",
        "    u\"\\u201d\": '\"',\n",
        "    u\"\\u2018\": \"'\",\n",
        "    u\"\\u2019\": \"'\",\n",
        "    \"  +\": \" \"\n",
        "}\n",
        "\n",
        "class GeneralConferenceDataset(Dataset):\n",
        "  def __init__(self, root=root_dir, video_file=\"frames.mp4\", text_file=\"text.txt\", frames_per_item=sample_frames):\n",
        "    self.root_dir = root\n",
        "    self.video_file = video_file\n",
        "    self.text_file = text_file\n",
        "    self.frames_per_item = frames_per_item\n",
        "    self.years = self._discover_folders(root_dir)\n",
        "    self.months = self._discover_folders(self.years)\n",
        "    self.talks = self._discover_folders(self.months)\n",
        "    self.tokenizer = spacy.load('en').tokenizer\n",
        "    self.transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Lambda(lambda img: img.transpose(0, 2)),\n",
        "        torchvision.transforms.ToPILImage(),\n",
        "        torchvision.transforms.Resize(224),                \n",
        "        torchvision.transforms.CenterCrop(224),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def _discover_folders(self, parent):\n",
        "    if type(parent) != list: # we're gonna flatten lists, so this makes that easier\n",
        "      parent = [parent]\n",
        "    return [os.path.join(p, d.name) for p in parent for d in os.scandir(p) if d.is_dir()]\n",
        "\n",
        "  def _load_text(self, talk_dir, video_frames):\n",
        "    \"\"\"\n",
        "    Loads the text in the given talk as a list of word2vec vectors.\n",
        "\n",
        "    However, it does this by assuming that the temporal length is best measured by the number of word pieces, not the number of characters or english words\n",
        "    This is an assumption that we'll need to revisit in the future.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(talk_dir, self.text_file), 'r') as f:\n",
        "      talk_text = f.read().lower()\n",
        "\n",
        "      # Because of the way that I downloaded the data, we need to separate sentences and replace crappy apostrophes.\n",
        "      for key in text_replacements:\n",
        "        talk_text = re.sub(key, text_replacements[key], talk_text)\n",
        "      \n",
        "      tokens = self.tokenizer(talk_text)\n",
        "      num_tokens = len(tokens)\n",
        "      desired_length = math.ceil((self.frames_per_item / video_frames) * num_tokens)\n",
        "      start_token = random.randint(0, num_tokens - desired_length)\n",
        "\n",
        "      token_sample = tokens[start_token:start_token + desired_length]\n",
        "      return token_sample\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "      Returns a random sample of the video at index, and the text we hope that it represents.\n",
        "    \"\"\"\n",
        "    # Load the video and get a sample of the frames. Video is of size [num_frames, h, w, c]\n",
        "    talk_dir = self.talks[index]\n",
        "    video, _, meta = torchvision.io.video.read_video(os.path.join(talk_dir, self.video_file), pts_unit=\"sec\")\n",
        "    num_frames = video.size(0)\n",
        "    length = self.frames_per_item\n",
        "    start_frame = random.randint( 0, num_frames - length )\n",
        "    frame_sample = video[start_frame:start_frame + length]\n",
        "\n",
        "    # Apply some transforms to the frames (but the same transform for every frame in video)\n",
        "    frame_sample = torch.stack([self.transforms(i) for i in frame_sample])\n",
        "\n",
        "    # Now get a chunk of text of hopefully comparable spot.\n",
        "    text_sample = self._load_text(talk_dir, num_frames)\n",
        "    return frame_sample, text_sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.talks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6HE1Qu6Nrme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = GeneralConferenceDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1AafyhsgGcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((2, 1, 0))\n",
        "    # mean = np.array([0.485, 0.456, 0.406])\n",
        "    # std = np.array([0.229, 0.224, 0.225])\n",
        "    # inp = std * inp + mean\n",
        "    # inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ww24TWKNlCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneralConferenceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GeneralConferenceModel, self).__init__()\n",
        "    self.feature_extracter = torchvision.models.resnet50(pretrained=True)\n",
        "    for param in self.feature_extracter.parameters():\n",
        "      param.requires_grad = False\n",
        "    num_f = self.feature_extracter.fc.in_features\n",
        "    self.feature_extracter.fc = nn.Linear(num_f, 100)\n",
        "\n",
        "    self.net = nn.LSTM(input_size=100, hidden_size=100, num_layers=2, batch_first=True) # Parameters should be tweaked, probably\n",
        "\n",
        "  def forward(self, frames, hidden=None, num_chunks=20):\n",
        "    # Extract features\n",
        "    print('Entering foward method:',frames.size())\n",
        "    chunk_size = len(frames) // num_chunks\n",
        "    features = []\n",
        "    for i in range(num_chunks):\n",
        "      frame_piece = frames[i*chunk_size:(i+1)*chunk_size]\n",
        "      frame_piece = frame_piece.cuda()\n",
        "      f = self.feature_extracter(frame_piece)\n",
        "      features.append(f)\n",
        "      frame_piece = frame_piece.cpu()\n",
        "    frame_features = torch.cat(features, dim=0)\n",
        "    print('Features have been extracted:',frame_features.size())\n",
        "    # LSTM expects them with shape (batch, time_sequence, input_size)\n",
        "    frame_features = frame_features.unsqueeze(0)\n",
        "    embeddings, hidden = self.net(frame_features, hidden)\n",
        "    print('Embeddings generated:',embeddings.size())\n",
        "    return embeddings, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3ELFRic6SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_emb(embeddings):\n",
        "  results = []\n",
        "  for e in embeddings.numpy():\n",
        "    top_choices = model_glove_wikipedia.similar_by_vector(e)\n",
        "    words, weights = zip(*top_choices)\n",
        "    weights = np.asarray(weights)\n",
        "    weights /= np.sum(weights)\n",
        "    word = np.random.choice(words, p=weights)\n",
        "    results.append(word)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_dSy1KMN7-2",
        "colab_type": "code",
        "outputId": "07577d48-2618-4528-8470-2e376cf9daf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# clean() and check_gpu() are located at the bottom of the notebook\n",
        "check_gpu()\n",
        "clean()\n",
        "model = GeneralConferenceModel()\n",
        "model = model.cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "check_gpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU RAM Free: 7MB | Used: 16273MB | Util 100% | Total 16280MB\n",
            "GPU RAM Free: 15441MB | Used: 839MB | Util   5% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQc2I6amuySc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load siamese lstm\n",
        "grader = nn.LSTM(input_size=100, hidden_size=200)\n",
        "# grader = grader.load()\n",
        "grader = grader.cuda().eval()\n",
        "objective = nn.MSELoss()\n",
        "def compute_manhattan(h1, h2):\n",
        "  return torch.exp(-torch.sum(torch.abs(h1 - h2), axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQYVYSZAaT7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs):\n",
        "  try:\n",
        "    for e in range(num_epochs):\n",
        "      for i in range(len(dataset)):\n",
        "        check_gpu()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vid, text = dataset[i]\n",
        "        check_gpu()\n",
        "        text_hat = []\n",
        "        hidden = None\n",
        "        for j in range(6):\n",
        "          check_gpu()\n",
        "          t, hidden = model(vid[j*100:(j+1)*100], hidden)\n",
        "          text_hat.append(t.squeeze(0))\n",
        "        text_hat = torch.cat(text_hat, dim=0)\n",
        "        check_gpu()\n",
        "        print('Full group processed:',text_hat.size())\n",
        "        if i % 5 == 0:\n",
        "          to_translate = text_hat.clone().detach().cpu().squeeze(0).squeeze(0)\n",
        "          results = translate_emb(to_translate)\n",
        "          print(results)\n",
        "\n",
        "        _, (h_hat, _) = grader(text_hat)\n",
        "        _, (h_text, _) = grader(text)\n",
        "        dist = compute_manhattan(h_hat, h_text).squeeze(0)\n",
        "        all_are_similar = torch.ones(dist.shape).cuda()\n",
        "        loss(dist, all_are_similar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "      break\n",
        "  except Exception:\n",
        "    __ITB__()\n",
        "    raise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Xr7n0YrsVH",
        "colab_type": "code",
        "outputId": "cb83e3e7-6668-4c31-c4e1-7b73fd3cfa7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "train(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU RAM Free: 15375MB | Used: 905MB | Util   6% | Total 16280MB\n",
            "GPU RAM Free: 15449MB | Used: 831MB | Util   5% | Total 16280MB\n",
            "GPU RAM Free: 15449MB | Used: 831MB | Util   5% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15373MB | Used: 907MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15373MB | Used: 907MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15373MB | Used: 907MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15373MB | Used: 907MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15371MB | Used: 909MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15371MB | Used: 909MB | Util   6% | Total 16280MB\n",
            "Full group processed: torch.Size([600, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['desormeaux', 'rickshaw', 'nzealand', 'demps', 'rhinoplasty', 'chipolopolo', 'safrican', 'safrican', 'demps', 'driveways', 'binglin', 'binglin', 'rickshaw', 'safrican', 'rickshaws', 'demps', 'nfp', 'demps', 'rickshaw', 'chipolopolo', 'webhouse', 'chipolopolo', 'rickshaws', 'binglin', 'rickshaw', 'chipolopolo', 'driveways', 'demps', 'demps', 'driveways', 'driveways', 'nfp', 'binglin', 'safrican', 'nfp', 'webhouse', 'chipolopolo', 'safrican', 'cng', 'rickshaws', 'safrican', 'webhouse', 'zanaco', 'chipolopolo', 'zanaco', 'rickshaw', 'demps', 'nfp', 'rickshaws', 'zanaco', 'demps', 'chipolopolo', 'safrican', 'rickshaw', 'binglin', 'zanaco', 'binglin', 'webhouse', 'rickshaws', 'webhouse', 'binglin', 'driveways', 'binglin', 'rhinoplasty', 'rickshaw', 'driveways', 'cng', 'driveways', 'rickshaw', 'binglin', 'rickshaw', 'webhouse', 'safrican', 'nfp', 'rickshaws', 'driveways', 'driveways', 'chipolopolo', 'chipolopolo', 'nfp', 'nfp', 'safrican', 'safrican', 'binglin', 'webhouse', 'chipolopolo', 'chipolopolo', 'safrican', 'driveways', 'driveways', 'binglin', 'rickshaws', 'rickshaw', 'rickshaw', 'chipolopolo', 'safrican', 'rhinoplasty', 'rickshaw', 'demps', 'demps', 'zanaco', 'nfp', 'webhouse', 'nfp', 'rickshaws', 'nfp', 'demps', 'driveways', 'chipolopolo', 'chipolopolo', 'rickshaws', 'webhouse', 'nfp', 'cng', 'webhouse', 'driveways', 'webhouse', 'rickshaw', 'safrican', 'rickshaw', 'webhouse', 'driveways', 'webhouse', 'driveways', 'rickshaws', 'webhouse', 'nfp', 'webhouse', 'webhouse', 'safrican', 'binglin', 'webhouse', 'nfp', 'binglin', 'nfp', 'webhouse', 'nfp', 'demps', 'webhouse', 'demps', 'cng', 'rickshaw', '125-mile', 'demps', 'rickshaws', 'demps', 'binglin', 'safrican', 'demps', 'nfp', 'chipolopolo', 'nfp', 'demps', 'safrican', 'nfp', 'demps', 'rickshaws', 'driveways', 'zanaco', 'binglin', 'demps', 'rickshaw', 'chipolopolo', 'nfp', 'demps', 'binglin', 'demps', 'chipolopolo', 'rickshaw', 'chipolopolo', 'driveways', 'rickshaw', 'driveways', 'webhouse', 'rickshaws', 'cng', 'webhouse', 'zanaco', 'webhouse', 'chipolopolo', 'rickshaws', 'demps', 'demps', 'chipolopolo', 'safrican', 'rickshaw', 'cng', 'chipolopolo', 'rickshaws', 'nfp', 'demps', 'driveways', 'webhouse', 'safrican', 'webhouse', 'webhouse', 'webhouse', 'webhouse', 'chipolopolo', 'zanaco', 'cng', 'chipolopolo', 'safrican', 'chipolopolo', 'binglin', 'nfp', 'safrican', 'webhouse', 'rickshaw', 'webhouse', 'zanaco', 'chipolopolo', 'binglin', 'webhouse', 'zanaco', 'driveways', 'driveways', 'binglin', 'cng', 'demps', 'safrican', 'rickshaw', 'zanaco', 'driveways', 'chipolopolo', 'chipolopolo', 'nfp', 'safrican', 'demps', 'demps', 'safrican', 'webhouse', 'safrican', 'chipolopolo', 'webhouse', 'safrican', 'chipolopolo', 'rickshaw', 'safrican', 'chipolopolo', 'safrican', 'webhouse', 'demps', 'demps', 'driveways', 'rickshaws', 'binglin', 'driveways', 'rickshaw', 'demps', 'demps', 'rhinoplasty', 'webhouse', 'webhouse', 'chipolopolo', 'rickshaw', 'binglin', 'zanaco', 'rickshaws', 'nfp', 'demps', 'binglin', 'demps', 'driveways', 'rickshaws', 'rickshaw', 'webhouse', 'rickshaw', 'rickshaw', 'rickshaw', 'binglin', 'chipolopolo', 'rickshaw', 'demps', 'rhinoplasty', 'webhouse', 'chipolopolo', 'driveways', 'webhouse', 'webhouse', 'rickshaw', 'nfp', 'driveways', 'binglin', 'safrican', 'safrican', 'webhouse', 'rickshaw', 'demps', 'rickshaws', 'nfp', 'webhouse', 'rickshaws', 'webhouse', 'rickshaws', 'sabulis', 'nfp', 'webhouse', 'chipolopolo', 'nfp', 'demps', 'webhouse', 'rickshaw', 'chipolopolo', 'webhouse', 'rickshaws', 'rickshaw', 'driveways', 'webhouse', 'chipolopolo', 'zanaco', 'binglin', 'rickshaws', 'demps', 'demps', 'rickshaws', 'chipolopolo', 'demps', 'cng', 'rhinoplasty', 'chipolopolo', 'driveways', 'rhinoplasty', 'chipolopolo', 'rickshaws', 'safrican', 'driveways', 'zanaco', 'driveways', 'rickshaw', 'chipolopolo', 'rickshaw', 'chipolopolo', 'safrican', 'chipolopolo', 'rickshaws', 'chipolopolo', 'cng', 'binglin', 'safrican', 'binglin', 'rickshaws', 'rickshaws', 'zanaco', 'zanaco', 'chipolopolo', 'chipolopolo', 'driveways', 'rickshaws', 'cng', 'driveways', 'webhouse', 'cng', 'driveways', 'nfp', 'safrican', 'rickshaws', 'webhouse', 'rhinoplasty', 'rickshaw', 'rickshaws', 'binglin', 'nfp', 'rickshaw', 'safrican', 'chipolopolo', 'sabulis', 'rhinoplasty', 'binglin', 'safrican', 'zanaco', 'driveways', 'nfp', 'rickshaw', 'rickshaws', 'zanaco', 'safrican', 'safrican', 'rickshaw', 'cng', 'rickshaws', 'chipolopolo', 'demps', 'webhouse', 'demps', 'cng', 'nfp', 'nfp', 'rickshaws', 'safrican', 'webhouse', 'driveways', 'nfp', 'webhouse', 'webhouse', 'rickshaw', 'safrican', 'webhouse', 'demps', 'safrican', 'nfp', 'rickshaws', 'chipolopolo', 'safrican', 'driveways', 'binglin', 'nfp', 'safrican', 'rickshaw', 'rickshaws', 'webhouse', 'rhinoplasty', 'chipolopolo', 'demps', 'webhouse', 'rickshaws', 'binglin', 'demps', 'rickshaw', 'rickshaw', 'rickshaws', 'demps', 'nfp', 'driveways', 'safrican', 'rickshaws', 'driveways', 'rhinoplasty', 'demps', 'zanaco', 'rickshaw', 'demps', 'zanaco', 'binglin', 'binglin', 'demps', 'chipolopolo', 'driveways', 'safrican', 'binglin', 'chipolopolo', 'safrican', 'demps', 'driveways', 'chipolopolo', 'chipolopolo', 'rickshaws', 'demps', 'safrican', 'rickshaws', 'rickshaw', 'rickshaws', 'chipolopolo', 'safrican', 'driveways', 'binglin', 'chipolopolo', 'zanaco', 'nfp', 'demps', 'nfp', 'rickshaw', 'driveways', 'nfp', 'chipolopolo', 'chipolopolo', 'chipolopolo', 'safrican', 'safrican', 'nfp', 'rickshaw', 'rickshaw', 'rhinoplasty', 'demps', 'demps', 'webhouse', 'nfp', 'rickshaw', 'webhouse', 'rickshaw', 'demps', 'nfp', 'safrican', 'chipolopolo', 'rickshaws', 'safrican', 'webhouse', 'nfp', 'webhouse', 'rickshaws', 'rickshaw', 'sabulis', 'zanaco', 'safrican', 'safrican', 'demps', 'nfp', 'nfp', 'cng', 'webhouse', 'safrican', 'safrican', 'rickshaws', 'nfp', 'rickshaws', 'safrican', 'nfp', 'zanaco', 'zanaco', 'rickshaws', 'rickshaw', 'webhouse', 'webhouse', 'rickshaws', 'rickshaw', 'driveways', 'zanaco', '125-mile', 'binglin', 'cng', 'webhouse', 'nfp', 'safrican', 'safrican', 'driveways', '125-mile', 'rickshaw', 'demps', 'chipolopolo', 'nfp', 'nfp', 'sabulis', 'binglin', 'safrican', 'demps', 'safrican', 'rickshaw', 'rickshaws', 'chipolopolo', 'rickshaws', 'driveways', 'nfp', 'safrican', 'cng', 'demps', 'rickshaw', 'nfp', 'webhouse', 'cng', 'nfp', 'cng', 'rhinoplasty', 'rickshaws', 'rickshaws', 'chipolopolo', 'webhouse', 'cng', 'nfp', 'demps', 'nfp', 'nfp', 'safrican', 'safrican', 'demps', 'driveways', 'rickshaws', 'demps', 'rickshaws', 'webhouse', 'webhouse', 'rickshaws', 'demps', 'chipolopolo', 'rickshaw', 'rickshaws', 'nfp', 'nfp', 'demps', 'chipolopolo', 'rickshaw', 'driveways', 'driveways', 'driveways', 'safrican', 'rickshaws', 'nfp', 'nfp', 'webhouse', 'driveways', 'nfp', 'nfp', 'nfp', 'rickshaw', 'safrican', 'binglin', 'webhouse', 'binglin', 'rickshaw', 'cng', 'webhouse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pcwtP_a9pk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}