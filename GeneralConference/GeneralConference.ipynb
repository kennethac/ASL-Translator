{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneralConference.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te3Bjkwhe2Ue",
        "colab_type": "text"
      },
      "source": [
        "This notebook uses data scraped from the churchofjesuschrist.org website with ASL language general conference talks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPsFv-ccwAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl -o gc.zip https://students.cs.byu.edu/~kac1995/2000.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOllGGoO0edW",
        "colab_type": "code",
        "outputId": "2997e8fe-d668-4fc3-dd0c-bb9cd729046b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "  1. Mount Google Drive (no need to check if already mounted, it does that for you)\n",
        "  2. \n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPYkGq9k0nZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf \"/content/gc\"\n",
        "!unzip \"/content/gdrive/My Drive/CS474 Final Project/GC/2000.zip\" > /dev/null\n",
        "!mv \"/content/users/guest/k/kac1995/dev/CS474-General-Conference-Downloader/gc\" \"/content/gc\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN_UowwLoUzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# root directory has a structure like \"year/month/talk/[video|text]\"\n",
        "root_dir = \"/content/gc\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgRirH6MHIbf",
        "colab_type": "code",
        "outputId": "d1e0c533-b78f-4065-cbbf-7d8f2de5a3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "!sudo apt install libavdevice-dev libavfilter-dev > /dev/null # Required to get av to install\n",
        "!pip3 install av # Required for torchvision to work with videos.\n",
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!pip3 install gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.6/dist-packages (6.2.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.3.1)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.10.36)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.13.36)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_UyQQ2HI4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, utils, datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchtext\n",
        "import spacy\n",
        "import gc\n",
        "import os\n",
        "import math\n",
        "import av\n",
        "import re\n",
        "import gensim.downloader as gensim_api\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FkduX1Wv_eK",
        "colab_type": "code",
        "outputId": "cc39c6f5-c8f6-400f-ed8f-5c9cafb745da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install gputil\n",
        "import GPUtil as GPU\n",
        "\n",
        "def clean():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "def check_gpu():\n",
        "  GPUs = GPU.getGPUs()\n",
        "  gpu = GPUs[0]\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "clean()\n",
        "check_gpu()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "GPU RAM Free: 16270MB | Used: 10MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvOPp1Jol5K",
        "colab_type": "code",
        "outputId": "21b58c5c-db61-41b8-ef6d-bdf70d9be34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "model_glove_wikipedia = gensim_api.load(\"glove-wiki-gigaword-100\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDfXDVZUplq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_length = len(model_glove_wikipedia[\"jesus\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0RMpJBKLAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_minutes = 1\n",
        "fps = 10\n",
        "sample_frames = sample_minutes * 60 * fps\n",
        "\n",
        "text_replacements = {\n",
        "    # end of paragraphs that may not have been done correctly\n",
        "    '\\.([^ ])': \". \\\\1\",\n",
        "    \"\\!([^ ])\": \"! \\\\1\",\n",
        "    \"\\?([^ ])\": \"? \\\\1\",\n",
        "    \"\\:([^ ])\": \": \\\\1\",\n",
        "    \n",
        "    # Some unicode chars that I know of\n",
        "    u\"\\u201c\": '\"',\n",
        "    u\"\\u201d\": '\"',\n",
        "    u\"\\u2018\": \"'\",\n",
        "    u\"\\u2019\": \"'\",\n",
        "    \"  +\": \" \"\n",
        "}\n",
        "\n",
        "class GeneralConferenceDataset(Dataset):\n",
        "  def __init__(self, root=root_dir, video_file=\"frames.mp4\", text_file=\"text.txt\", frames_per_item=sample_frames):\n",
        "    self.root_dir = root\n",
        "    self.video_file = video_file\n",
        "    self.text_file = text_file\n",
        "    self.frames_per_item = frames_per_item\n",
        "    self.years = self._discover_folders(root_dir)\n",
        "    self.months = self._discover_folders(self.years)\n",
        "    self.talks = self._discover_folders(self.months)\n",
        "    self.tokenizer = spacy.load('en').tokenizer\n",
        "    self.transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Lambda(lambda img: img.transpose(0, 2)),\n",
        "        torchvision.transforms.ToPILImage(),\n",
        "        torchvision.transforms.Resize(224),                \n",
        "        torchvision.transforms.CenterCrop(224),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def _discover_folders(self, parent):\n",
        "    if type(parent) != list: # we're gonna flatten lists, so this makes that easier\n",
        "      parent = [parent]\n",
        "    return [os.path.join(p, d.name) for p in parent for d in os.scandir(p) if d.is_dir()]\n",
        "\n",
        "  def _load_text(self, talk_dir, video_frames):\n",
        "    \"\"\"\n",
        "    Loads the text in the given talk as a list of word2vec vectors.\n",
        "\n",
        "    However, it does this by assuming that the temporal length is best measured by the number of word pieces, not the number of characters or english words\n",
        "    This is an assumption that we'll need to revisit in the future.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(talk_dir, self.text_file), 'r') as f:\n",
        "      talk_text = f.read().lower()\n",
        "\n",
        "      # Because of the way that I downloaded the data, we need to separate sentences and replace crappy apostrophes.\n",
        "      for key in text_replacements:\n",
        "        talk_text = re.sub(key, text_replacements[key], talk_text)\n",
        "      \n",
        "      tokens = self.tokenizer(talk_text)\n",
        "      num_tokens = len(tokens)\n",
        "      desired_length = math.ceil((self.frames_per_item / video_frames) * num_tokens)\n",
        "      start_token = random.randint(0, num_tokens - desired_length)\n",
        "\n",
        "      token_sample = tokens[start_token:start_token + desired_length]\n",
        "      return token_sample\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "      Returns a random sample of the video at index, and the text we hope that it represents.\n",
        "    \"\"\"\n",
        "    # Load the video and get a sample of the frames. Video is of size [num_frames, h, w, c]\n",
        "    talk_dir = self.talks[index]\n",
        "    video, _, meta = torchvision.io.video.read_video(os.path.join(talk_dir, self.video_file), pts_unit=\"sec\", start_pts=5.0)\n",
        "    num_frames = video.size(0)\n",
        "    length = self.frames_per_item\n",
        "    start_frame = random.randint( 0, num_frames - length )\n",
        "    frame_sample = video[start_frame:start_frame + length]\n",
        "\n",
        "    # Apply some transforms to the frames (but the same transform for every frame in video)\n",
        "    frame_sample = torch.stack([self.transforms(i) for i in frame_sample])\n",
        "\n",
        "    # Now get a chunk of text of hopefully comparable spot.\n",
        "    text_sample = self._load_text(talk_dir, num_frames)\n",
        "    return frame_sample, text_sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.talks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6HE1Qu6Nrme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = GeneralConferenceDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1AafyhsgGcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((2, 1, 0))\n",
        "    # mean = np.array([0.485, 0.456, 0.406])\n",
        "    # std = np.array([0.229, 0.224, 0.225])\n",
        "    # inp = std * inp + mean\n",
        "    # inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ww24TWKNlCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneralConferenceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GeneralConferenceModel, self).__init__()\n",
        "    self.feature_extracter = torchvision.models.resnet50(pretrained=True)\n",
        "    for param in self.feature_extracter.parameters():\n",
        "      param.requires_grad = False\n",
        "    num_f = self.feature_extracter.fc.in_features\n",
        "    self.feature_extracter.fc = nn.Linear(num_f, 100)\n",
        "\n",
        "    self.net = nn.LSTM(input_size=100, hidden_size=100, num_layers=2, batch_first=True) # Parameters should be tweaked, probably\n",
        "\n",
        "  def forward(self, frames, hidden=None, num_chunks=20):\n",
        "    # Extract features\n",
        "    print('Entering foward method:',frames.size())\n",
        "    chunk_size = len(frames) // num_chunks\n",
        "    features = []\n",
        "    for i in range(num_chunks):\n",
        "      frame_piece = frames[i*chunk_size:(i+1)*chunk_size]\n",
        "      frame_piece = frame_piece.cuda()\n",
        "      f = self.feature_extracter(frame_piece)\n",
        "      features.append(f)\n",
        "      frame_piece = frame_piece.cpu()\n",
        "    frame_features = torch.cat(features, dim=0)\n",
        "    print('Features have been extracted:',frame_features.size())\n",
        "    # LSTM expects them with shape (batch, time_sequence, input_size)\n",
        "    frame_features = frame_features.unsqueeze(0)\n",
        "    embeddings, hidden = self.net(frame_features, hidden)\n",
        "    print('Embeddings generated:',embeddings.size())\n",
        "    return embeddings, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3ELFRic6SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_emb(embeddings):\n",
        "  results = []\n",
        "  for e in embeddings.numpy():\n",
        "    top_choices = model_glove_wikipedia.similar_by_vector(e)\n",
        "    words, weights = zip(*top_choices)\n",
        "    weights = np.asarray(weights)\n",
        "    weights /= np.sum(weights)\n",
        "    word = np.random.choice(words, p=weights)\n",
        "    results.append(word)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w5VJJINXSWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_emb(tokens):\n",
        "  # tokenizer = spacy.load('en').tokenizer\n",
        "  return [ model_glove_wikipedia[i] for i in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGtAXxn2Yg79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce7a8095-b76a-4819-c904-087ceac6e4e7"
      },
      "source": [
        "tokenizer2 = spacy.load('en').tokenizer\n",
        "ts2 = tokenizer2(\"I'm going to be a dad.\".lower())\n",
        "# [ model_glove_wikipedia[i] for i in ts2 ]\n",
        "torch.stack([torch.tensor(model_glove_wikipedia[str(i)]) for i in ts2]).size()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_dSy1KMN7-2",
        "colab_type": "code",
        "outputId": "40d212c9-ab00-4faa-a7ed-ef0bfb74138e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# clean() and check_gpu() are located at the bottom of the notebook\n",
        "check_gpu()\n",
        "clean()\n",
        "model = GeneralConferenceModel()\n",
        "model = model.cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "check_gpu()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU RAM Free: 16270MB | Used: 10MB | Util   0% | Total 16280MB\n",
            "GPU RAM Free: 15439MB | Used: 841MB | Util   5% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQc0XggNVTUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingLookupModule(nn.Module):\n",
        "  def forward(self, l):\n",
        "    l = l.int().numpy()\n",
        "    # print(type(l))\n",
        "    # print(\"l size: {}\".format(l.shape))\n",
        "    res = torch.tensor(embeddings[l]).float().transpose(0,1)\n",
        "    # print(\"Looked up size: {}\".format(res.size()))\n",
        "    return res.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXcgjeC8VM8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fat_grader = nn.Sequential(\n",
        "    EmbeddingLookupModule(),\n",
        "    nn.LSTM(input_size=100, hidden_size=200)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQc2I6amuySc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load siamese lstm\n",
        "grader = nn.LSTM(input_size=100, hidden_size=200)\n",
        "# grader = grader.load()\n",
        "grader = grader.cuda().eval()\n",
        "objective = nn.MSELoss()\n",
        "def compute_manhattan(h1, h2):\n",
        "  return torch.exp(-torch.sum(torch.abs(h1 - h2), axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e08ZuemxTfA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Because in training the MaLSTM I unthinkingly saved out the entire sequential\n",
        "# model I have to recreate it in fat_grader, restore the params we want, \n",
        "# then pull out the LSTM portion of the fat_grader.\n",
        "grader_model_file = \"/content/gdrive/My Drive/CS474 Final Project/MaLSTM/e49_l0.17679482698440552.mod\"\n",
        "mk, uk = fat_grader.load_state_dict(torch.load(grader_model_file))\n",
        "grader = [ i for i in fat_grader.modules() ][2].cuda()\n",
        "assert len(mk) == len(uk) and len(uk) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQYVYSZAaT7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs):\n",
        "  try:\n",
        "    for e in range(num_epochs):\n",
        "      for i in range(len(dataset)):\n",
        "        check_gpu()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vid, text = dataset[i]\n",
        "        check_gpu()\n",
        "        text_hat = []\n",
        "        hidden = None\n",
        "        for j in range(6):\n",
        "          check_gpu()\n",
        "          t, hidden = model(vid[j*100:(j+1)*100], hidden)\n",
        "          text_hat.append(t.squeeze(0))\n",
        "        text_hat = torch.cat(text_hat, dim=0)\n",
        "        check_gpu()\n",
        "        print('Full group processed:',text_hat.size())\n",
        "        if i % 5 == 0:\n",
        "          to_translate = text_hat.clone().detach().cpu().squeeze(0).squeeze(0)\n",
        "          results = translate_emb(to_translate)\n",
        "          print(results)\n",
        "\n",
        "        # text = make_emb(text)\n",
        "        print(text)\n",
        "        print(\"Text hat: {}\".format(text_hat.size()), flush=True)\n",
        "        # print(\"text truth: {}\".format(text.size()), flush=True)\n",
        "        text = torch.stack([ torch.tensor(model_glove_wikipedia[str(i)]) if str(i) in model_glove_wikipedia.vocab else torch.zeros((100)) for i in text])\n",
        "        text_hat_size = text_hat.size()\n",
        "        text_size = text.size()\n",
        "        \n",
        "        text_hat = text_hat.view(text_hat_size[0], 1, text_hat_size[1]).cuda()\n",
        "        print(\"Text hat: {}\".format(text_hat.size()), flush=True)\n",
        "        \n",
        "        text = text.view(text_size[0], 1, text_size[1]).cuda()\n",
        "        \n",
        "        print(\"text truth: {}\".format(text.size()), flush=True)\n",
        "        _, (h_hat, _) = grader(text_hat)\n",
        "        _, (h_text, _) = grader(text)\n",
        "        dist = compute_manhattan(h_hat, h_text).squeeze(0)\n",
        "        all_are_similar = torch.ones(dist.shape).cuda()\n",
        "        loss = objective(dist, all_are_similar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "      break\n",
        "  except Exception:\n",
        "    __ITB__()\n",
        "    raise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Xr7n0YrsVH",
        "colab_type": "code",
        "outputId": "e9d9d0a8-6f9a-4410-e8fb-2a818eb93f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "train(4)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU RAM Free: 15355MB | Used: 925MB | Util   6% | Total 16280MB\n",
            "GPU RAM Free: 15429MB | Used: 851MB | Util   5% | Total 16280MB\n",
            "GPU RAM Free: 15429MB | Used: 851MB | Util   5% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15353MB | Used: 927MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15351MB | Used: 929MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15349MB | Used: 931MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15349MB | Used: 931MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15347MB | Used: 933MB | Util   6% | Total 16280MB\n",
            "Entering foward method: torch.Size([100, 3, 224, 224])\n",
            "Features have been extracted: torch.Size([100, 100])\n",
            "Embeddings generated: torch.Size([1, 100, 100])\n",
            "GPU RAM Free: 15345MB | Used: 935MB | Util   6% | Total 16280MB\n",
            "Full group processed: torch.Size([600, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['penetrated', 'epicenter', 'crater', 'crater', 'workings', 'workings', 'mesopotamia', 'scandinavia', 'periphery', 'shadowy', 'infiltrated', 'periphery', 'shadowy', 'scandinavia', 'western', 'mesopotamia', 'depths', 'underworld', 'depths', 'tirah', 'periphery', 'nubia', 'mesopotamia', 'depths', 'underworld', 'mesopotamia', 'scandinavia', 'periphery', 'penetrated', 'penetrated', 'infiltrated', 'tirah', 'mesopotamia', 'underworld', 'periphery', 'tirah', 'depths', 'penetrated', 'depths', 'shadowy', 'scandinavia', 'mesopotamia', 'penetrated', 'mesopotamia', 'scandinavia', 'giulia', 'lies', 'nubia', 'tirah', 'scandinavia', 'mesopotamia', 'mesopotamia', 'periphery', 'western', 'depths', 'scandinavia', 'underworld', 'scandinavia', 'tirah', 'depths', 'penetrated', 'periphery', 'underworld', 'scandinavia', 'underworld', 'periphery', 'shadowy', 'depths', 'depths', 'workings', 'underworld', 'penetrated', 'mesopotamia', 'mesopotamia', 'shadowy', 'periphery', 'workings', 'workings', 'workings', 'depths', 'penetrated', 'shadowy', 'mesopotamia', 'giulia', 'tirah', 'shadowy', 'underworld', 'depths', 'mesopotamia', 'depths', 'shadowy', 'periphery', 'giulia', 'periphery', 'workings', 'penetrated', 'periphery', 'western', 'depths', 'periphery', 'periphery', 'scandinavia', 'depths', 'western', 'periphery', 'penetrated', 'mesopotamia', 'penetrated', 'western', 'depths', 'underworld', 'workings', 'mesopotamia', 'scandinavia', 'tirah', 'giulia', 'tirah', 'periphery', 'periphery', 'depths', 'penetrated', 'workings', 'mesopotamia', 'depths', 'underworld', 'giulia', 'giulia', 'shadowy', 'penetrated', 'tirah', 'periphery', 'penetrated', 'tirah', 'underworld', 'tirah', 'western', 'mesopotamia', 'depths', 'mesopotamia', 'mesopotamia', 'workings', 'periphery', 'nubia', 'giulia', 'depths', 'underworld', 'scandinavia', 'western', 'shadowy', 'periphery', 'scandinavia', 'depths', 'workings', 'workings', 'depths', 'underworld', 'scandinavia', 'shadowy', 'periphery', 'giulia', 'shadowy', 'giulia', 'western', 'workings', 'scandinavia', 'tirah', 'nubia', 'scandinavia', 'workings', 'tirah', 'penetrated', 'mesopotamia', 'penetrated', 'nubia', 'depths', 'western', 'giulia', 'depths', 'depths', 'periphery', 'periphery', 'workings', 'depths', 'tirah', 'workings', 'mesopotamia', 'shadowy', 'western', 'workings', 'shadowy', 'scandinavia', 'penetrated', 'depths', 'scandinavia', 'giulia', 'depths', 'shadowy', 'underworld', 'underworld', 'workings', 'mesopotamia', 'mesopotamia', 'giulia', 'tirah', 'depths', 'scandinavia', 'underworld', 'scandinavia', 'scandinavia', 'western', 'mesopotamia', 'periphery', 'periphery', 'underworld', 'workings', 'penetrated', 'underworld', 'tirah', 'tirah', 'periphery', 'scandinavia', 'underworld', 'workings', 'shadowy', 'scandinavia', 'shadowy', 'tirah', 'scandinavia', 'penetrated', 'shadowy', 'workings', 'mesopotamia', 'workings', 'scandinavia', 'depths', 'western', 'workings', 'underworld', 'workings', 'depths', 'scandinavia', 'nubia', 'giulia', 'periphery', 'scandinavia', 'tirah', 'giulia', 'workings', 'periphery', 'penetrated', 'mesopotamia', 'periphery', 'penetrated', 'scandinavia', 'underworld', 'western', 'workings', 'giulia', 'workings', 'underworld', 'underworld', 'periphery', 'mesopotamia', 'workings', 'giulia', 'scandinavia', 'periphery', 'penetrated', 'penetrated', 'underworld', 'mesopotamia', 'giulia', 'scandinavia', 'scandinavia', 'mesopotamia', 'tirah', 'mesopotamia', 'scandinavia', 'tirah', 'shadowy', 'workings', 'underworld', 'workings', 'workings', 'shadowy', 'scandinavia', 'periphery', 'mesopotamia', 'depths', 'workings', 'mesopotamia', 'western', 'periphery', 'mesopotamia', 'scandinavia', 'western', 'workings', 'periphery', 'periphery', 'periphery', 'scandinavia', 'mesopotamia', 'penetrated', 'western', 'periphery', 'western', 'giulia', 'western', 'depths', 'underworld', 'scandinavia', 'tirah', 'western', 'mesopotamia', 'underworld', 'shadowy', 'depths', 'western', 'periphery', 'mesopotamia', 'giulia', 'periphery', 'shadowy', 'depths', 'depths', 'mesopotamia', 'underworld', 'penetrated', 'scandinavia', 'giulia', 'depths', 'periphery', 'mesopotamia', 'mesopotamia', 'nubia', 'underworld', 'workings', 'nubia', 'scandinavia', 'giulia', 'shadowy', 'mesopotamia', 'periphery', 'underworld', 'nubia', 'workings', 'giulia', 'tirah', 'scandinavia', 'periphery', 'periphery', 'western', 'periphery', 'depths', 'periphery', 'periphery', 'workings', 'western', 'tirah', 'shadowy', 'scandinavia', 'workings', 'mesopotamia', 'periphery', 'nubia', 'mesopotamia', 'giulia', 'depths', 'depths', 'shadowy', 'tirah', 'scandinavia', 'giulia', 'scandinavia', 'depths', 'penetrated', 'scandinavia', 'western', 'tirah', 'western', 'depths', 'shadowy', 'scandinavia', 'periphery', 'depths', 'underworld', 'depths', 'mesopotamia', 'mesopotamia', 'underworld', 'shadowy', 'underworld', 'tirah', 'nubia', 'shadowy', 'western', 'shadowy', 'shadowy', 'periphery', 'giulia', 'giulia', 'mesopotamia', 'underworld', 'workings', 'scandinavia', 'shadowy', 'periphery', 'shadowy', 'giulia', 'giulia', 'underworld', 'western', 'mesopotamia', 'mesopotamia', 'periphery', 'tirah', 'tirah', 'tirah', 'tirah', 'underworld', 'underworld', 'periphery', 'underworld', 'depths', 'workings', 'tirah', 'western', 'scandinavia', 'depths', 'mesopotamia', 'periphery', 'underworld', 'giulia', 'penetrated', 'scandinavia', 'workings', 'penetrated', 'penetrated', 'depths', 'periphery', 'nubia', 'depths', 'shadowy', 'mesopotamia', 'periphery', 'tirah', 'mesopotamia', 'scandinavia', 'western', 'underworld', 'shadowy', 'workings', 'periphery', 'tirah', 'western', 'periphery', 'shadowy', 'nubia', 'mesopotamia', 'mesopotamia', 'mesopotamia', 'underworld', 'depths', 'depths', 'underworld', 'underworld', 'periphery', 'western', 'underworld', 'tirah', 'depths', 'shadowy', 'scandinavia', 'scandinavia', 'shadowy', 'shadowy', 'underworld', 'shadowy', 'tirah', 'mesopotamia', 'mesopotamia', 'scandinavia', 'periphery', 'underworld', 'scandinavia', 'tirah', 'giulia', 'scandinavia', 'periphery', 'tirah', 'shadowy', 'shadowy', 'workings', 'giulia', 'tirah', 'scandinavia', 'workings', 'penetrated', 'tirah', 'depths', 'workings', 'mesopotamia', 'underworld', 'mesopotamia', 'giulia', 'workings', 'shadowy', 'shadowy', 'nubia', 'workings', 'periphery', 'underworld', 'depths', 'underworld', 'western', 'giulia', 'workings', 'penetrated', 'underworld', 'depths', 'scandinavia', 'workings', 'periphery', 'depths', 'workings', 'shadowy', 'scandinavia', 'giulia', 'workings', 'mesopotamia', 'giulia', 'workings', 'shadowy', 'workings', 'giulia', 'giulia', 'penetrate', 'giulia', 'penetrated', 'shadowy', 'mesopotamia', 'tirah', 'shadowy', 'depths', 'underworld', 'mesopotamia', 'shadowy', 'periphery', 'shadowy', 'giulia', 'penetrated', 'mesopotamia', 'scandinavia', 'periphery', 'western', 'shadowy', 'scandinavia', 'scandinavia', 'mesopotamia', 'giulia', 'depths', 'workings', 'depths', 'western', 'shadowy', 'tirah', 'mesopotamia', 'mesopotamia', 'periphery', 'workings', 'western', 'tirah', 'nubia', 'underworld', 'periphery', 'underworld', 'periphery', 'shadowy', 'periphery', 'periphery', 'giulia', 'scandinavia', 'workings', 'underworld', 'underworld', 'shadowy', 'workings', 'underworld', 'mesopotamia', 'depths', 'penetrated', 'penetrated', 'western', 'mesopotamia', 'underworld', 'workings', 'underworld', 'scandinavia', 'scandinavia', 'scandinavia']\n",
            ", according to their riches and their chances for learning; yea, some were ignorant because of their poverty, and others did receive great learning because of their riches\" (3 ne. 6: 12). furthermore, malevolent, human social structures have included, in the past, tragic constraints like slavery and concentration camps. nevertheless, we are to do what we can within our allotted \"acreage,\" while still using whatever stretch there may be in any tethers. within what is allotted to us, we can have spiritual contentment. paul described it as \"godliness with contentment,\" signifying the adequate presence of attributes such as love, hope, meekness, patience, and submissiveness (1 tim. 6: 6). yet there are other fixed limitations in life. for instance, some have allotments\n",
            "Text hat: torch.Size([600, 100])\n",
            "Text hat: torch.Size([600, 1, 100])\n",
            "text truth: torch.Size([156, 1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pcwtP_a9pk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}